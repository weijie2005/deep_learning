import torch
import matplotlib.pyplot as plt

'''
一、人工神经网络（Artificial Neural Network, ANN）简称神经网络NN：
    1.什么是神经网络
        神经网络NN：是一种模拟人脑神经元工作原理的计算模型。它由多个人工神经元（或节点）组成，这些神经元通过连接（或边）进行通信。
                   每个神经元接收来自其他神经元的输入，对这些输入进行加权求和，然后通过一个激活函数来产生输出。

        神经网络可以用于解决各种问题，如分类、回归、聚类等。它们在模式识别、数据挖掘、人工智能等领域有广泛的应用。

    2.人工神经网络的结构
        神经网络是由多个神经元组成的网络，神经网络的结构可以分为输入层、隐藏层和输出层。
        神经元neuron：
            1.神经元是人工神经网络的基本单位，它接收来自其他神经元的输入，对这些输入进行加权求和，然后通过一个激活函数来产生输出。
            2.每个神经元都有一个唯一的标识符，用于在神经网络中引用它。
        输入层：接收外部输入数据。
        隐藏层：对输入数据进行处理和转换，加权和+激活函数。
        输出层：产生最终的输出结果。        
        物理点：
            1.同一层的神经元之间是没有连接的
            2.第N层的神经元只与第N-1层的所有神经元相连，就是全连神经网络,full connected
            3.第N-1层神经元的输出作为第N层神经元的输入
            4.每个连接都有一个权重值(w权重系数和b偏置系数)
        
    3、神经网络的工作原理：
        1.前向传播
            前向传播是人工神经网络的基本操作，它从输入层开始，通过隐藏层，最终到达输出层。
            在每个神经元中，输入被加权求和，然后通过激活函数进行处理，产生输出。
            这个过程被称为前向传播，因为信息从输入层流向输出层。

        2.反向传播
            反向传播是人工神经网络训练的关键步骤，它基于梯度下降算法。
            反向传播从输出层开始，计算输出层的误差，然后通过隐藏层反向传播误差，更新每个连接的权重。
            这个过程被称为反向传播，因为误差从输出层流向输入层。

二、激活函数(Activation Function)
        1.激活函数：是神经网络中用于引入非线性性质的函数，对每层神经元的输出进行非线性变换。以此让神经网络可以拟合各种曲线。
        2.常用的激活函数包括:对数和指数函数，sigmoid 函数、ReLU 函数、tanh 函数等。
        3.不加激活函数：
            1.函数本身是线性的：f(x)=W0 * X0 + W1 * X1 + ... + Wn * Xn + B ，无法拟合复杂的典线。
            2.如果不使用激活函数，神经网络就成为了一个简单的线性模型，无法拟合复杂的非线性函数。
            3.在实际应用中，为了提高神经网络的性能，通常会在隐藏层中使用激活函数。

        4.微软网站神经网络拟合过程：http://playground.tensorflow.org ,
            1.可以在网站上调整神经网络的参数，如层数、神经元数量、激活函数等，观察神经网络的拟合过程。
            2.可以通过调整参数，观察神经网络的性能，如准确率、损失函数等。
            3.可以在网站上上传自己的数据集，进行神经网络的训练和测试。

        5.常用的激活函数：
            1.sigmoid 函数(又称logistic函数)：将输入映射到 (0, 1) 之间，常用在二分类问题中。
               1. 公式：f(x) = 1 / (1 + exp(-x))
               2. 范围：(0, 1)
               3. 导数：f'(x) = f(x) * (1 - f(x))
               4. 优点：简单、易于实现，输出在 (0, 1) 之间，以0.5为中心，方便进行概率解释。
               5. 缺点：其导数范围在(0, 0.25)之间，当输入值在(-6, 6)之间时，导数接近0，产生梯度消失问题。在5层之内的神经网络中，梯度消失问题会更加严重。
               6. 解决方法：使用批量归一化（Batch Normalization）等技术来缓解梯度消失问题。

            2. tanh 函数：将输入映射到 (-1, 1) 之间，常用在隐藏层中。
                1. 公式：f(x) = (1 - exp(-2x)) / (1 + exp(-2x))
                2. 范围：(-1, 1)
                3. 导数：f'(x) = 1 - f(x)^2
                4. 优点：简单、易于实现，输出在 (-1, 1) 之间，中心对称,以0为中心，方便进行梯度计算。其梯度比sigmoid大，收敛速度也快一些
                5. 缺点：其导数范围在(0, 1)之间，当输入值在(-3, 3)之间时，导数接近0，产生致梯度消失问题，不会在5层内产生梯度消失问题。
                6. 解决方法：使用批量归一化（Batch Normalization）等技术来缓解梯度消失问题。

            3. ReLU (Rectified Linear Unit) 函数：将输入大于 0 的部分直接输出，小于等于 0 的部分输出 0，常用在隐藏层中。                
                1. 公式：f(x) = max(0, x)
                2. 范围：[0, ∞)
                3. 导数：f'(x) = 1 (x > 0), 0 (x ≤ 0)
                4. 优点：简单、易于实现，计算速度快，避免了 sigmoid 函数的梯度消失问题。ReLu函数是目前使用最广泛的激活函数。
                5. 缺点：输出不在 (-1, 1) 之间，不以0为中心，输出值的范围受到输入值的影响，输入小于0时，神经元死亡。
                6. 解决方法：使用批量归一化（Batch Normalization）等技术来缓解梯度消失问题。
                7. 优化ReLu:leaky ReLU
                    1. 公式：f(x) = max(0.01x, x)  ,在x<0时，输出x乘以一个系统0.01，这样y值就不为0了，在x>=0时，输出x
                    2. 范围：[0, ∞)
                    3. 导数：f'(x) = 0.01 (x ≤ 0), 1 (x > 0)
                    4. 优点：解决了 ReLU 函数在输入小于 0 时的梯度消失问题，保持了 ReLU 函数的计算效率。
                    5. 缺点：引入了一个新的超参数 α，需要手动调整。

            4. softmax 函数：将网络的输出层的加权和即logit通过softmax函数，映射到 (0, 1) 之间的概率值，常用在多分类问题中，并且只用的输出层中。
                1. 公式：f(xi) = exp(xi) / Σ(exp(xj)) (j = 1, 2, ..., n)
                2. 范围：(0, 1)
                3. 导数：f'(xi) = f(xi) * (1 - f(xi))
                4. 优点：简单、易于实现，输出在 (0, 1) 之间，以0为中心，方便进行概率解释。
                5. 缺点：
                6. 解决方法：使用批量归一化（Batch Normalization）等技术来缓解梯度消失问题。

            5. 选择激活函数：
                1. 二分类问题：sigmoid 函数,指用在输出层中
                2. 多分类问题：softmax 函数,指用在输出层中
                3. 隐藏层：ReLU 函数或 tanh 函数
                4. 输出层：sigmoid 激活函数，多分类问题使用softmax函数
                5. 回归类激活函数：无，不需要，直接输入即输出
                6. 指数激活函数：sigmoid, tanh
                7. 分段激活函数：ReLU, [ReLU增加超参后的变形：leaky ReLU ，PReLU，RReLU,ELU]
                8. 隐层主要使用ReLU函数，当期效果不好，造成神经元死亡问题时，使用leaky ReLU函数。
                9. 回归问题使用identity函数，即不使用激活函数，直接输出。


三、神经网络初始化参数：
    1.w权重参数：
        1.平均分布：w参数从[-1/sqrt(n), 1/sqrt(n)]之间随机初始化，其中n是上一层的神经元数量。
        2.正态分布：w参数从均值为0，标准差为1/sqrt(n)的正态分布中随机初始化。
        3.HE(kaiming)初始化：
            1. 平均分布HE初始化：w参数是[-sqrt(6/fan_in), sqrt(6/fan_in)]平均初始化，其中fan_in是输入神经元数量。
            2. 正态分布HE初始化：w参数是[sqrt(2/fan_in)]的正态分布初始化，其中fan_in是输入神经元数量。

        4.xavier初始化：
            1. 平均分布xavier初始化：w参数是[-sqrt(6/fan_in+fan_out), sqrt(6/fan_in+fan_out)]平均初始化，其中fan_in是输入神经元数量，fan_out是输出神经元数量。
            2. 正态分布xavier初始化：w参数是[sqrt(2/fan_in+fan_out)]的正态分布初始化，其中fan_in是输入神经元数量，fan_out是输出神经元数量。

    2.b偏置参数：
        1.全0初始化：b参数初始化为0。
        2.全1初始化：b参数初始化为1。
        3.固定值初始化：b参数初始化为一个固定的标量值，如0.1。


四、神经网络的创建过程：
    1. 定义神经网络的层：
        1. 输入层：根据输入数据的维度定义输入层的神经元数量。
        2. 隐藏层：根据任务需求和经验定义隐藏层的神经元数量和层数，每个隐藏层使用ReLU激活函数。
        3. 输出层：根据输出数据的维度定义输出层的神经元数量，根据任务需求选择合适的激活函数。
    2. 初始化参数：
        1. 对每个层的权重参数w进行初始化，这里选择xavier初始化。
        2. 对每个层的偏置参数b进行初始化，这里选择全0初始化。
    3. 定义前向传播：
        1. 对输入数据进行前向传播，通过每个层的权重参数和偏置参数进行计算，得到输出结果。
    4. 定义损失函数：
        1. 根据任务需求选择合适的损失函数，如均方误差损失、交叉熵损失等。
    5. 定义优化器：
        1. 选择合适的优化器，如随机梯度下降（SGD）、Adam等。                
'''

'''
    各激活函数演示：sigmoid, tanh, ReLU, softmax
'''

def sigmod_demo():
    x=torch.linspace(-20,20,1000)
    y=torch.sigmoid(x)
    plt.plot(x,y)
    plt.grid(True)
    plt.show()

    #导函数
    y=y*(1-y)
    plt.plot(x,y)
    plt.grid(True)
    plt.show()

def tanh_demo():
    x=torch.linspace(-20,20,1000)
    y=torch.tanh(x)
    plt.plot(x,y)
    plt.grid(True)
    plt.show()

    #导函数
    y=1-y**2
    plt.plot(x,y)
    plt.grid(True)
    plt.show()

def relu_demo():
    x=torch.linspace(-20,20,1000)
    y=torch.relu(x)
    plt.plot(x,y)
    plt.title('ReLU Function')
    plt.xlabel('x')
    plt.ylabel('y')
    plt.grid(True)
    plt.show()

    #导数演示
    x=torch.linspace(-20,20,1000,requires_grad=True)
    y=torch.relu(x).sum().backward()
    plt.plot(x.grad)
    plt.title('ReLU Function Gradient')
    plt.xlabel('x')
    plt.ylabel('y')
    plt.grid(True)
    plt.show()


def softmax_demo():
    #定义一个logits张量
    logits=torch.tensor([0.2,0.02,0.15,1.3,0.5,0.06,1.1,0.05,3.75])
    print("logits:",logits)

    #对logits应用softmax函数
    probs=torch.softmax(logits,dim=0)
    print("softmax probabilities:",probs)

if __name__=='__main__':
    #sigmod_demo()
    #tanh_demo()
    #relu_demo()
    softmax_demo()
