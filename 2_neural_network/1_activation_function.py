import torch
import matplotlib.pyplot as plt

'''
一、人工神经网络（Artificial Neural Network, ANN）简称神经网络NN：
    1.什么是神经网络
        神经网络NN：是一种模拟人脑神经元工作原理的计算模型。它由多个人工神经元（或节点）组成，这些神经元通过连接（或边）进行通信。
                   每个神经元接收来自其他神经元的输入，对这些输入进行加权求和，然后通过一个激活函数来产生输出。

        神经网络可以用于解决各种问题，如分类、回归、聚类等。它们在模式识别、数据挖掘、人工智能等领域有广泛的应用。

    2.人工神经网络的结构
        神经网络是由多个神经元组成的网络，神经网络的结构可以分为输入层、隐藏层和输出层。
        神经元neuron：
            1.神经元是人工神经网络的基本单位，它接收来自其他神经元的输入，对这些输入进行加权求和，然后通过一个激活函数来产生输出。
            2.每个神经元都有一个唯一的标识符，用于在神经网络中引用它。
        输入层：接收外部输入数据。
        隐藏层：对输入数据进行处理和转换，加权和+激活函数。
        输出层：产生最终的输出结果。        
        物理点：
            1.同一层的神经元之间是没有连接的
            2.第N层的神经元只与第N-1层的所有神经元相连，就是全连神经网络,full connected
            3.第N-1层神经元的输出作为第N层神经元的输入
            4.每个连接都有一个权重值(w权重系数和b偏置系数)
        
    3、神经网络的工作原理：
        1.前向传播
            前向传播是人工神经网络的基本操作，它从输入层开始，通过隐藏层，最终到达输出层。
            在每个神经元中，输入被加权求和，然后通过激活函数进行处理，产生输出。
            这个过程被称为前向传播，因为信息从输入层流向输出层。

        2.反向传播
            反向传播是人工神经网络训练的关键步骤，它基于梯度下降算法。
            反向传播从输出层开始，计算输出层的误差，然后通过隐藏层反向传播误差，更新每个连接的权重。
            这个过程被称为反向传播，因为误差从输出层流向输入层。

二、激活函数(Activation Function)
        1.激活函数：是神经网络中用于引入非线性性质的函数，对每层神经元的输出进行非线性变换。以此让神经网络可以拟合各种曲线。
        2.常用的激活函数包括:对数和指数函数，sigmoid 函数、ReLU 函数、tanh 函数等。
        3.不加激活函数：
            1.函数本身是线性的：f(x)=W0 * X0 + W1 * X1 + ... + Wn * Xn + B ，无法拟合复杂的典线。
            2.如果不使用激活函数，神经网络就成为了一个简单的线性模型，无法拟合复杂的非线性函数。
            3.在实际应用中，为了提高神经网络的性能，通常会在隐藏层中使用激活函数。

        4.微软网站神经网络拟合过程：http://playground.tensorflow.org ,
            1.可以在网站上调整神经网络的参数，如层数、神经元数量、激活函数等，观察神经网络的拟合过程。
            2.可以通过调整参数，观察神经网络的性能，如准确率、损失函数等。
            3.可以在网站上上传自己的数据集，进行神经网络的训练和测试。

        5.常用的激活函数：
            1.sigmoid 函数(又称logistic函数)：将输入映射到 (0, 1) 之间，常用在二分类问题中。
               1. 公式：f(x) = 1 / (1 + exp(-x))
               2. 范围：(0, 1)
               3. 导数：f'(x) = f(x) * (1 - f(x))
               4. 优点：简单、易于实现，输出在 (0, 1) 之间，以0.5为中心，方便进行概率解释。
               5. 缺点：其导数范围在(0, 0.25)之间，当输入值在(-6, 6)之间时，导数接近0，产生梯度消失问题。在5层之内的神经网络中，梯度消失问题会更加严重。
               6. 解决方法：使用批量归一化（Batch Normalization）等技术来缓解梯度消失问题。

            2. tanh 函数：将输入映射到 (-1, 1) 之间，常用在隐藏层中。
                1. 公式：f(x) = (1 - exp(-2x)) / (1 + exp(-2x))
                2. 范围：(-1, 1)
                3. 导数：f'(x) = 1 - f(x)^2
                4. 优点：简单、易于实现，输出在 (-1, 1) 之间，中心对称,以0为中心，方便进行梯度计算。其梯度比sigmoid大，收敛速度也快一些
                5. 缺点：其导数范围在(0, 1)之间，当输入值在(-3, 3)之间时，导数接近0，产生致梯度消失问题，不会在5层内产生梯度消失问题。
                6. 解决方法：使用批量归一化（Batch Normalization）等技术来缓解梯度消失问题。

            3. ReLU (Rectified Linear Unit) 函数：将输入大于 0 的部分直接输出，小于等于 0 的部分输出 0，常用在隐藏层中。                
                1. 公式：f(x) = max(0, x)
                2. 范围：[0, ∞)
                3. 导数：f'(x) = 1 (x > 0), 0 (x ≤ 0)
                4. 优点：简单、易于实现，计算速度快，避免了 sigmoid 函数的梯度消失问题。ReLu函数是目前使用最广泛的激活函数。
                5. 缺点：输出不在 (-1, 1) 之间，不以0为中心，输出值的范围受到输入值的影响，输入小于0时，神经元死亡。
                6. 解决方法：使用批量归一化（Batch Normalization）等技术来缓解梯度消失问题。
                7. 优化ReLu:leaky ReLU
                    1. 公式：f(x) = max(0.01x, x)  ,在x<0时，输出x乘以一个系统0.01，这样y值就不为0了，在x>=0时，输出x
                    2. 范围：[0, ∞)
                    3. 导数：f'(x) = 0.01 (x ≤ 0), 1 (x > 0)
                    4. 优点：解决了 ReLU 函数在输入小于 0 时的梯度消失问题，保持了 ReLU 函数的计算效率。
                    5. 缺点：引入了一个新的超参数 α，需要手动调整。

            4. softmax 函数：将网络的输出层的加权和即logit通过softmax函数，映射到 (0, 1) 之间的概率值，常用在多分类问题中，并且只用的输出层中。
                1. 公式：f(xi) = exp(xi) / Σ(exp(xj)) (j = 1, 2, ..., n)
                2. 范围：(0, 1)
                3. 导数：f'(xi) = f(xi) * (1 - f(xi))
                4. 优点：简单、易于实现，输出在 (0, 1) 之间，以0为中心，方便进行概率解释。
                5. 缺点：
                6. 解决方法：使用批量归一化（Batch Normalization）等技术来缓解梯度消失问题。

            5. 选择激活函数：
                1. 二分类问题：sigmoid 函数,指用在输出层中
                2. 多分类问题：softmax 函数,指用在输出层中
                3. 隐藏层：ReLU 函数或 tanh 函数
                4. 输出层：sigmoid 激活函数，多分类问题使用softmax函数
                5. 回归类激活函数：无，不需要，直接输入即输出
                6. 指数激活函数：sigmoid, tanh
                7. 分段激活函数：ReLU, [ReLU增加超参后的变形：leaky ReLU ，PReLU，RReLU,ELU]
                8. 隐层主要使用ReLU函数，当期效果不好，造成神经元死亡问题时，使用leaky ReLU函数。
                9. 回归问题使用identity函数，即不使用激活函数，直接输出。


三、神经网络初始化参数：
    1.w权重参数：
        1.平均分布：w参数从[-1/sqrt(n), 1/sqrt(n)]之间随机初始化，其中n是上一层的神经元数量。
        2.正态分布：w参数从均值为0，标准差为1/sqrt(n)的正态分布中随机初始化。
        3.HE(kaiming)初始化：
            1. 平均分布HE初始化：w参数是[-sqrt(6/fan_in), sqrt(6/fan_in)]平均初始化，其中fan_in是输入神经元数量。
            2. 正态分布HE初始化：w参数是[sqrt(2/fan_in)]的正态分布初始化，其中fan_in是输入神经元数量。

        4.xavier初始化：
            1. 平均分布xavier初始化：w参数是[-sqrt(6/fan_in+fan_out), sqrt(6/fan_in+fan_out)]平均初始化，其中fan_in是输入神经元数量，fan_out是输出神经元数量。
            2. 正态分布xavier初始化：w参数是[sqrt(2/fan_in+fan_out)]的正态分布初始化，其中fan_in是输入神经元数量，fan_out是输出神经元数量。

    2.b偏置参数：
        1.全0初始化：b参数初始化为0。
        2.全1初始化：b参数初始化为1。
        3.固定值初始化：b参数初始化为一个固定的标量值，如0.1。


四、神经网络的创建过程：
    1. 定义神经网络的层：
        1. 输入层：根据输入数据的维度定义输入层的神经元数量。
        2. 隐藏层：根据任务需求和经验定义隐藏层的神经元数量和层数，每个隐藏层使用ReLU激活函数。
        3. 输出层：根据输出数据的维度定义输出层的神经元数量，根据任务需求选择合适的激活函数。
    2. 初始化参数：
        1. 对每个层的权重参数w进行初始化，这里选择xavier初始化。
        2. 对每个层的偏置参数b进行初始化，这里选择全0初始化。
    3. 定义前向传播：
        1. 对输入数据进行前向传播，通过每个层的权重参数和偏置参数进行计算，得到输出结果。
    4. 定义损失函数：
        1. 根据任务需求选择合适的损失函数，如均方误差损失、交叉熵损失等。
    5. 定义优化器：
        1. 选择合适的优化器，如随机梯度下降（SGD）、Adam等。       

五、神经网络的优缺点：
    1. 优点：
        1. 简单、易于实现，精度高。
        2. 可以处理非线性问题，如分类、回归等。
        3. 可以自动学习特征表示，无需手动设计特征工程。
        4. 可以处理大规模数据集，训练效率高。

    2. 缺点：
        1. 容易陷入局部最优解，导致模型性能下降。
        2. 训练时间长，尤其是在大规模数据集上。
        3. 对输入数据的缩放敏感，需要进行归一化处理。
        4. 对缺失值敏感，需要进行缺失值处理。
        5. 网络结构复杂，需要调整超参数，如学习率、批量大小、迭代次数等。
        6. 小数据集上的表现不佳，容易过拟合。
        7. 黑箱模型：神经网络是一种黑箱模型，即无法直接解释模型的决策过程，只用于用户端场景，有不确定性；不适合解释性要求高的场景如：金融，军事科技，医疗等。
        8. 解释性差：神经网络的决策过程是基于权重参数的复杂计算，无法直接解释为人类可理解的规则。

六、神经网络的应用场景：
    1. 图像分类：如MNIST数据集、CIFAR-10数据集等。
    2. 物体检测：如YOLO、Faster R-CNN等。
    3. 语音识别：如Google Speech Recognition、Microsoft Bing Voice等。
    4. 自然语言处理：如情感分析、机器翻译等。
    5. 推荐系统：如Netflix、Amazon等。
    6. 金融预测：如股票价格预测、风险评估等。
    7. 医疗诊断：如疾病分类、医疗图像分析等。
    8. 自动驾驶：如self-driving cars等。

七、神经网络的损失函数(loss function)：在文献中有其它名称如：代价函数(cost function)、目标函数(objective function)、误差函数(error function)等。
    1.什么是损失函数？
        1. 损失函数是衡量模型参数(w,b)质量好坏的函数。
        1. 损失函数是神经网络模型训练过程中用于评估模型预测结果与真实标签之间差异的函数。
        2. 损失函数的目标是最小化模型预测结果与真实标签之间的差异，从而提高模型的预测准确性。

    2. 分类问题常用损失函数：
        1. 交叉熵损失函数（Cross-Entropy Loss）：用于多分类问题，衡量模型预测的概率分布与真实标签的差异。
        2. 二进制交叉熵损失函数（Binary Cross-Entropy Loss）：用于二分类问题，衡量模型预测的概率与真实标签的差异。

    3. 回归问题常用损失函数：
        1. 均方误差损失函数（Mean Squared Error Loss）：用于回归问题，衡量模型预测值与真实值之间的差异的平方均值。
        2. 平均绝对误差损失函数（Mean Absolute Error Loss）：用于回归问题，衡量模型预测值与真实值之间的差异的绝对值均值。

'''

'''
    各激活函数演示：sigmoid, tanh, ReLU, softmax
'''

def sigmod_demo():
    x=torch.linspace(-20,20,1000)
    y=torch.sigmoid(x)
    plt.plot(x,y)
    plt.grid(True)
    plt.show()

    #导函数
    y=y*(1-y)
    plt.plot(x,y)
    plt.grid(True)
    plt.show()

def tanh_demo():
    x=torch.linspace(-20,20,1000)
    y=torch.tanh(x)
    plt.plot(x,y)
    plt.grid(True)
    plt.show()

    #导函数
    y=1-y**2
    plt.plot(x,y)
    plt.grid(True)
    plt.show()

def relu_demo():
    x=torch.linspace(-20,20,1000)
    y=torch.relu(x)
    plt.plot(x,y)
    plt.title('ReLU Function')
    plt.xlabel('x')
    plt.ylabel('y')
    plt.grid(True)
    plt.show()

    #导数演示
    x=torch.linspace(-20,20,1000,requires_grad=True)
    y=torch.relu(x).sum().backward()
    plt.plot(x.grad)
    plt.title('ReLU Function Gradient')
    plt.xlabel('x')
    plt.ylabel('y')
    plt.grid(True)
    plt.show()


def softmax_demo():
    #定义一个logits张量
    logits=torch.tensor([0.2,0.02,0.15,1.3,0.5,0.06,1.1,0.05,3.75])
    print("logits:",logits)

    #对logits应用softmax函数
    probs=torch.softmax(logits,dim=0)
    print("softmax probabilities:",probs)

if __name__=='__main__':
    #sigmod_demo()
    #tanh_demo()
    #relu_demo()
    softmax_demo()
